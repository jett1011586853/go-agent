
===================================
== NVIDIA NIM for Text Reranking ==
===================================

NVIDIA Release 1.8.0
Model: nvidia/llama-3.2-nv-rerankqa-1b-v2

Container image Copyright (c) 2016-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
The NIM container is governed by the NVIDIA Software License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and the Product-Specific Terms for NVIDIA AI Products (found at https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/).

Use of this model is governed by the NVIDIA Community Model License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).

ADDITIONAL INFORMATION: Llama 3.2 Community License Agreement (https://www.llama.com/llama3_2/license/). Built with Llama.

A copy of this license can be found under /opt/nim/LICENSE.
Third Party Software Attributions and Licenses can be found under /opt/nim/acknowledgements.txt.

Overriding NIM_LOG_LEVEL: replacing NIM_LOG_LEVEL=unset with NIM_LOG_LEVEL=INFO
HF_HOME is set to /opt/nim/.cache/huggingface
docker : INFO 2026-02-28 09:22:21.818 lib.rs:203] File: tokenizer_config.json found in cache: "/opt/nim/.cache/ngc/hub/
models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_config.json"
At line:2 char:1
+ docker logs rerank-nim 2>&1 | Out-File -Encoding utf8 d:\go-agent\.ru ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (INFO 2026-02-28...er_config.json":String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError
 
INFO 2026-02-28 09:22:19.046 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/opt/nim/start_server.d', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
NIM_MODEL_PROFILE is set to bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:19.719 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/opt/nim/start_server.d', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2026-02-28 09:22:21.772 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:21.772 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:21.772 nim_sdk.py:299] Using the profile selected by the profile selector: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:21.772 nim_sdk.py:308] Downloading manifest profile: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:21.818 public.rs:52] Skipping download, using cached copy of file: tokenizer_config.json at path:
 "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_
config.json"
INFO 2026-02-28 09:22:21.834 lib.rs:203] File: model.plan found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia-
-llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/model.plan"
INFO 2026-02-28 09:22:21.834 public.rs:52] Skipping download, using cached copy of file: model.plan at path: "/opt/nim/
.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/model.plan"
INFO 2026-02-28 09:22:21.901 nim_sdk.py:328] Using the workspace specified during init: /opt/nim/workspace
INFO 2026-02-28 09:22:21.902 nim_sdk.py:341] Materializing workspace to: /opt/nim/workspace
INFO 2026-02-28 09:22:27.030 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2026-02-28 09:22:27.315 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:27.316 nim_sdk.py:294] Using the profile specified by the user: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:27.316 nim_sdk.py:308] Downloading manifest profile: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:21.848 lib.rs:203] File: memory_footprint.json found in cache: "/opt/nim/.cache/ngc/hub/models--n
im--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/memory_footprint.json"
INFO 2026-02-28 09:22:21.848 public.rs:52] Skipping download, using cached copy of file: memory_footprint.json at path:
 "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/memory_f
ootprint.json"
INFO 2026-02-28 09:22:21.861 lib.rs:203] File: checksums.blake3 found in cache: "/opt/nim/.cache/ngc/hub/models--nim--n
vidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/checksums.blake3"
INFO 2026-02-28 09:22:21.861 public.rs:52] Skipping download, using cached copy of file: checksums.blake3 at path: "/op
t/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/checksums.bla
ke3"
INFO 2026-02-28 09:22:21.874 lib.rs:203] File: tokenizer.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvi
dia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:22:21.874 public.rs:52] Skipping download, using cached copy of file: tokenizer.json at path: "/opt/
nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:22:21.887 lib.rs:203] File: special_tokens_map.json found in cache: "/opt/nim/.cache/ngc/hub/models-
-nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_tokens_map.json"
INFO 2026-02-28 09:22:21.887 public.rs:52] Skipping download, using cached copy of file: special_tokens_map.json at pat
h: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_
tokens_map.json"
INFO 2026-02-28 09:22:21.900 lib.rs:203] File: metadata.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvid
ia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/metadata.json"
INFO 2026-02-28 09:22:21.900 public.rs:52] Skipping download, using cached copy of file: metadata.json at path: "/opt/n
im/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/metadata.json"
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:22:27Z INFO: nimlib - Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/u
sr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dis
t-packages']
2026-02-28T09:22:27Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:27Z INFO: nimlib.nim_sdk - Using the profile specified by the user: bbf794644e63fe68abeffb0dae1c4f7341
22ace0350f827c8935cc75e80df265
2026-02-28T09:22:27Z INFO: nimlib.nim_sdk - Downloading manifest profile: bbf794644e63fe68abeffb0dae1c4f734122ace0350f8
27c8935cc75e80df265
INFO 2026-02-28 09:22:27.337 lib.rs:203] File: special_tokens_map.json found in cache: "/opt/nim/.cache/ngc/hub/models-
-nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_tokens_map.json"
INFO 2026-02-28 09:22:27.337 public.rs:52] Skipping download, using cached copy of file: special_tokens_map.json at pat
h: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_
tokens_map.json"
INFO 2026-02-28 09:22:27.354 lib.rs:203] File: tokenizer.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvi
dia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:22:27.354 public.rs:52] Skipping download, using cached copy of file: tokenizer.json at path: "/opt/
nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:22:27.370 lib.rs:203] File: memory_footprint.json found in cache: "/opt/nim/.cache/ngc/hub/models--n
im--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/memory_footprint.json"
INFO 2026-02-28 09:22:27.370 public.rs:52] Skipping download, using cached copy of file: memory_footprint.json at path:
 "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/memory_f
ootprint.json"
INFO 2026-02-28 09:22:27.386 lib.rs:203] File: model.plan found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia-
-llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/model.plan"
INFO 2026-02-28 09:22:27.386 public.rs:52] Skipping download, using cached copy of file: model.plan at path: "/opt/nim/
.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/model.plan"
INFO 2026-02-28 09:22:27.402 lib.rs:203] File: tokenizer_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--n
im--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_config.json"
INFO 2026-02-28 09:22:27.402 public.rs:52] Skipping download, using cached copy of file: tokenizer_config.json at path:
 "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_
config.json"
INFO 2026-02-28 09:22:27.418 lib.rs:203] File: checksums.blake3 found in cache: "/opt/nim/.cache/ngc/hub/models--nim--n
vidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/checksums.blake3"
INFO 2026-02-28 09:22:27.418 public.rs:52] Skipping download, using cached copy of file: checksums.blake3 at path: "/op
t/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/checksums.bla
ke3"
INFO 2026-02-28 09:22:27.436 lib.rs:203] File: metadata.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvid
ia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/metadata.json"
INFO 2026-02-28 09:22:27.436 public.rs:52] Skipping download, using cached copy of file: metadata.json at path: "/opt/n
im/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp8-lwpgyxqy1w/metadata.json"
2026-02-28T09:22:27Z INFO: nimlib.nim_sdk - Using the workspace specified during init: /opt/nim/workspace
2026-02-28T09:22:27Z INFO: nimlib.nim_sdk - Materializing workspace to: /opt/nim/workspace
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:22:34Z INFO: nemo-retriever-reranking-triton-gen - Loaded tokenizer from /opt/nim/workspace/tokenizer
2026-02-28T09:22:44Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:22:44Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:22:44Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:22:44Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:22:44Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:22:44Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:22:44Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - Available TRT engine profiles: 
  - (max_batch_size=1, max_seq_length=8192)
  - (max_batch_size=8, max_seq_length=8192)
  - (max_batch_size=16, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=1024)
  - (max_batch_size=30, max_seq_length=2048)
  - (max_batch_size=30, max_seq_length=4096)
INFO 2026-02-28 09:22:27.437 nim_sdk.py:328] Using the workspace specified during init: /opt/nim/workspace
INFO 2026-02-28 09:22:27.437 nim_sdk.py:341] Materializing workspace to: /opt/nim/workspace
INFO 2026-02-28 09:22:32.002 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2026-02-28 09:22:34.231 repository.py:154] Loaded tokenizer from /opt/nim/workspace/tokenizer
WARNING 2026-02-28 09:22:44.499 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:22:44.499 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:22:44.499 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:22:44.499 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:22:44.499 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:22:44.500 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:22:44.500 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
INFO 2026-02-28 09:22:44.578 tensor_rt_model_builder.py:151] Available TRT engine profiles: 
  - (max_batch_size=1, max_seq_length=8192)
  - (max_batch_size=8, max_seq_length=8192)
  - (max_batch_size=16, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=1024)
  - (max_batch_size=30, max_seq_length=2048)
  - (max_batch_size=30, max_seq_length=4096)
INFO 2026-02-28 09:22:44.578 repository.py:256] Selected TensorRT engine profile: profile_index=3 inputs=[TensorInfo(name='input_ids', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192), min_shapes=(1, 2)), TensorInfo(name='attention_mask', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192), min_shapes=(1, 2))] outputs=[TensorInfo(name='logits', dtype='float32', shape=(1, 1), opt_shapes=(), max_shapes=(), min_shapes=())]
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - Selected TensorRT engine profile: profile_index=3 inpu
ts=[TensorInfo(name='input_ids', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192), min_shapes
=(1, 2)), TensorInfo(name='attention_mask', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192),
 min_shapes=(1, 2))] outputs=[TensorInfo(name='logits', dtype='float32', shape=(1, 1), opt_shapes=(), max_shapes=(), mi
n_shapes=())]
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - Setting number of models for '_nvidia_llama_3_2_nv_rer
ankqa_1b_v2_model' to: 1
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - Using TensorRT max_batch_size: 30
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - Python model successfully generated and written to: /o
pt/nim/tmp/run/triton-model-repository/_nvidia_llama_3_2_nv_rerankqa_1b_v2_model/1/model.py
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - BLS model successfully generated and written to: /opt/
nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/model.py
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - Successfully saved model service config to /opt/nim/tm
p/run/triton-model-repository/service_config.yaml
2026-02-28T09:22:44Z INFO: nemo-retriever-reranking-triton-gen - Sucessfully generated Triton Model Repository at /opt/
nim/tmp/run/triton-model-repository
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:22:45Z INFO: nimlib - Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/u
sr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dis
t-packages']
2026-02-28T09:22:45Z INFO: nimlib.nim_inference_api_builder.api - Serving endpoints:
  0.0.0.0:8000/v1/triton-inference-statistics (GET)
  0.0.0.0:8000/v1/models (GET)
  0.0.0.0:8000/v1/ranking (POST)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/metrics (GET)
  0.0.0.0:8000/v1/license (GET)
  0.0.0.0:8000/v1/metadata (GET)
  0.0.0.0:8000/v1/manifest (GET)
2026-02-28T09:22:45Z INFO: nimlib.nim_inference_api_builder.api - {'message': 'Starting HTTP Inference server', 'port':
 8000, 'workers_count': 8, 'host': '0.0.0.0', 'log_level': 'info', 'SSL': 'disabled'}
2026-02-28T09:22:45Z INFO: uvicorn.error - Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2026-02-28T09:22:45Z INFO: uvicorn.error - Started parent process [248]
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [317]
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [316]
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
INFO 2026-02-28 09:22:44.580 tensor_rt_model_builder.py:228] Setting number of models for '_nvidia_llama_3_2_nv_rerankqa_1b_v2_model' to: 1
INFO 2026-02-28 09:22:44.580 tensor_rt_model_builder.py:231] Using TensorRT max_batch_size: 30
INFO 2026-02-28 09:22:44.580 tensor_rt_model_builder.py:244] Python model successfully generated and written to: /opt/nim/tmp/run/triton-model-repository/_nvidia_llama_3_2_nv_rerankqa_1b_v2_model/1/model.py
INFO 2026-02-28 09:22:44.761 bls_model_builder.py:75] BLS model successfully generated and written to: /opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/model.py
INFO 2026-02-28 09:22:44.782 repository.py:529] Successfully saved model service config to /opt/nim/tmp/run/triton-model-repository/service_config.yaml
INFO 2026-02-28 09:22:44.782 repository.py:531] Sucessfully generated Triton Model Repository at /opt/nim/tmp/run/triton-model-repository
INFO 2026-02-28 09:22:45.429 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
I0228 09:22:45.579233 247 pinned_memory_manager.cc:277] "Pinned memory pool is created at '0x204c00000' with size 268435456"
I0228 09:22:45.579362 247 cuda_memory_manager.cc:107] "CUDA memory pool is created on device 0 with size 67108864"
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [310]
I0228 09:22:45.594644 247 model_lifecycle.cc:473] "loading: _nvidia_llama_3_2_nv_rerankqa_1b_v2_model:1"
I0228 09:22:45.594773 247 model_lifecycle.cc:473] "loading: nvidia_llama_3_2_nv_rerankqa_1b_v2:1"
I0228 09:22:45.650195 247 tensorrt.cc:65] "TRITONBACKEND_Initialize: tensorrt"
I0228 09:22:45.650253 247 tensorrt.cc:75] "Triton TRITONBACKEND API version: 1.19"
I0228 09:22:45.650261 247 tensorrt.cc:81] "'tensorrt' TRITONBACKEND API version: 1.19"
I0228 09:22:45.650267 247 tensorrt.cc:105] "backend configuration:\n{\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"version-compatible\":\"true\",\"default-max-batch-size\":\"4\"}}"
I0228 09:22:45.651296 247 tensorrt.cc:231] "TRITONBACKEND_ModelInitialize: _nvidia_llama_3_2_nv_rerankqa_1b_v2_model (version 1)"
INFO 2026-02-28 09:22:45.772 http_api.py:52] Serving endpoints:
  0.0.0.0:8000/v1/triton-inference-statistics (GET)
  0.0.0.0:8000/v1/models (GET)
  0.0.0.0:8000/v1/ranking (POST)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/metrics (GET)
  0.0.0.0:8000/v1/license (GET)
  0.0.0.0:8000/v1/metadata (GET)
  0.0.0.0:8000/v1/manifest (GET)
INFO 2026-02-28 09:22:45.772 http_api.py:73] {'message': 'Starting HTTP Inference server', 'port': 8000, 'workers_count': 8, 'host': '0.0.0.0', 'log_level': 'info', 'SSL': 'disabled'}
INFO 2026-02-28 09:22:45.924 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:47.262 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.262 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:47.271 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.271 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:47.273 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.273 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.273 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:47.273 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:47.295 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.295 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:47.300 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.300 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:47.315 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.315 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
INFO 2026-02-28 09:22:47.323 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:22:47.323 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [314]
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [313]
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [312]
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [311]
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:22:47Z INFO: uvicorn.error - Started server process [315]
2026-02-28T09:22:47Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:22:47Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:22:47Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: bbf794644e63
fe68abeffb0dae1c4f734122ace0350f827c8935cc75e80df265
2026-02-28T09:22:47Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:22:47Z INFO: uvicorn.error - Application startup complete.
I0228 09:22:49.014773 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_0 (CPU device 0)"
I0228 09:22:49.014891 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_1 (CPU device 0)"
I0228 09:22:49.015079 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_2 (CPU device 0)"
I0228 09:22:49.015265 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_3 (CPU device 0)"
I0228 09:22:49.015291 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_4 (CPU device 0)"
I0228 09:22:49.015411 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_5 (CPU device 0)"
I0228 09:22:49.015486 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_6 (CPU device 0)"
I0228 09:22:49.015612 247 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_7 (CPU device 0)"
INFO 2026-02-28 09:22:49.312 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:49.337 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:49.346 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:49.402 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:49.410 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:49.519 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:49.551 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:22:49.585 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:22:50Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_6', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_0', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_3', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_4', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_7', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_2', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_5', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"tokenizer_template":{"string_value":"question:{query} \
\n \\n passage:{passage}"},"reranking_model_name":{"string_value":"_nvidia_llama_3_2_nv_rerankqa_1b_v2_model"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_1', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:22:51Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:22:52Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:22:54Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:22:58Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:06Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
I0228 09:23:06.739574 674 pb_stub.cc:2075]  Non-graceful termination detected. 
I0228 09:23:06.783028 658 pb_stub.cc:2075] I0228 09:23:06.783017 639 pb_stub.cc:2075]  Non-graceful termination detecte
d.  Non-graceful termination detected. 

I0228 09:23:06.819456 644 pb_stub.cc:2075] I0228 09:23:06.819450 630 pb_stub.cc:2075] I0228 09:23:06.819482 689 pb_stub
.cc:2075] I0228 09:23:06.819480 706 pb_stub.cc:2075] I0228 09:23:06.819482 629 pb_stub.cc:2075]    Non-graceful termina
tion detected. Non-graceful termination detected.  Non-graceful termination detected.  Non-graceful termination detecte
d. Non-graceful termination detected. 




2026-02-28T09:23:10Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:14Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:18Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:22Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:26Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:30Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:34Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:38Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:42Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:46Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:50Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:54Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:23:58Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:02Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:06Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:10Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:14Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:18Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:22Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:26Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:24:30Z INFO: uvicorn.access - 172.17.0.1:60108 - "GET /v1/health/ready HTTP/1.1" 503
I0228 09:22:52.643100 247 model_lifecycle.cc:849] "successfully loaded 'nvidia_llama_3_2_nv_rerankqa_1b_v2'"
