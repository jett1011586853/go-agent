
===================================
== NVIDIA NIM for Text Reranking ==
===================================

NVIDIA Release 1.8.0
Model: nvidia/llama-3.2-nv-rerankqa-1b-v2

Container image Copyright (c) 2016-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
The NIM container is governed by the NVIDIA Software License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and the Product-Specific Terms for NVIDIA AI Products (found at https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/).

Use of this model is governed by the NVIDIA Community Model License Agreement (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).

ADDITIONAL INFORMATION: Llama 3.2 Community License Agreement (https://www.llama.com/llama3_2/license/). Built with Llama.

A copy of this license can be found under /opt/nim/LICENSE.
Third Party Software Attributions and Licenses can be found under /opt/nim/acknowledgements.txt.

Overriding NIM_LOG_LEVEL: replacing NIM_LOG_LEVEL=unset with NIM_LOG_LEVEL=INFO
HF_HOME is set to /opt/nim/.cache/huggingface
INFO 2026-02-28 09:40:53.358 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/opt/nim/start_server.d', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2026-02-28 09:40:53.619 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
NIM_MODEL_PROFILE is set to 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:40:53.804 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/opt/nim/start_server.d', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2026-02-28 09:40:54.575 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:40:54.575 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:40:54.575 nim_sdk.py:299] Using the profile selected by the profile selector: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:40:54.575 nim_sdk.py:308] Downloading manifest profile: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:22.444 nim_sdk.py:328] Using the workspace specified during init: /opt/nim/workspace
INFO 2026-02-28 09:52:22.446 nim_sdk.py:341] Materializing workspace to: /opt/nim/workspace
INFO 2026-02-28 09:52:25.448 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2026-02-28 09:52:25.823 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:25.823 nim_sdk.py:294] Using the profile specified by the user: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:25.824 nim_sdk.py:308] Downloading manifest profile: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
docker : INFO 2026-02-28 09:40:53.619 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4a
a05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
At line:2 char:1
+ docker logs rerank-nim 2>&1 | Out-File -Encoding utf8 d:\go-agent\.ru ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : NotSpecified: (INFO 2026-02-28...92e8d5af6abe91c:String) [], RemoteException
    + FullyQualifiedErrorId : NativeCommandError
 
INFO 2026-02-28 09:40:54.586 lib.rs:203] File: tokenizer.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvi
dia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:40:54.586 public.rs:52] Skipping download, using cached copy of file: tokenizer.json at path: "/opt/
nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:40:54.592 lib.rs:203] File: tokenizer_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--n
im--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_config.json"
INFO 2026-02-28 09:40:54.592 public.rs:52] Skipping download, using cached copy of file: tokenizer_config.json at path:
 "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_
config.json"
INFO 2026-02-28 09:40:54.598 tokio.rs:919] "nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:l40sx1-trt-fp16-k5no-6dx4q": fetchin
g filemap from: https://api.ngc.nvidia.com/v2/org/nim/team/nvidia/models/llama-3.2-nv-rerankqa-1b-v2/l40sx1-trt-fp16-k5
no-6dx4q/files
INFO 2026-02-28 09:40:54.622 lib.rs:203] File: special_tokens_map.json found in cache: "/opt/nim/.cache/ngc/hub/models-
-nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_tokens_map.json"
INFO 2026-02-28 09:40:54.622 public.rs:52] Skipping download, using cached copy of file: special_tokens_map.json at pat
h: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_
tokens_map.json"
INFO 2026-02-28 09:40:57.930 tokio.rs:814] Downloaded filename: memory_footprint.json to blob: "/opt/nim/.cache/ngc/hub
/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/blobs/d8b5e5ef992b951bc30e99eee9c4b4dc"
INFO 2026-02-28 09:40:58.085 tokio.rs:814] Downloaded filename: checksums.blake3 to blob: "/opt/nim/.cache/ngc/hub/mode
ls--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/blobs/0ba08d2ed7d56d2cb596460328226ed9"
INFO 2026-02-28 09:40:58.119 tokio.rs:814] Downloaded filename: metadata.json to blob: "/opt/nim/.cache/ngc/hub/models-
-nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/blobs/cc40dad4a497d7c7c65b7f3ace9dfd5a"
INFO 2026-02-28 09:52:22.441 tokio.rs:814] Downloaded filename: model.plan to blob: "/opt/nim/.cache/ngc/hub/models--ni
m--nvidia--llama-3.2-nv-rerankqa-1b-v2/blobs/84660b5ba279fb24fb3a7d5deb1d6703-6"
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:52:25Z INFO: nimlib - Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/u
sr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dis
t-packages']
2026-02-28T09:52:25Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:25Z INFO: nimlib.nim_sdk - Using the profile specified by the user: 4aa05527bed7c338689c9418e81d029fa0
3a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:25Z INFO: nimlib.nim_sdk - Downloading manifest profile: 4aa05527bed7c338689c9418e81d029fa03a2b58569ae
dda792e8d5af6abe91c
INFO 2026-02-28 09:52:25.839 lib.rs:203] File: checksums.blake3 found in cache: "/opt/nim/.cache/ngc/hub/models--nim--n
vidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/checksums.blake3"
INFO 2026-02-28 09:52:25.839 public.rs:52] Skipping download, using cached copy of file: checksums.blake3 at path: "/op
t/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/checksums.bl
ake3"
INFO 2026-02-28 09:52:25.845 lib.rs:203] File: model.plan found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvidia-
-llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/model.plan"
INFO 2026-02-28 09:52:25.845 public.rs:52] Skipping download, using cached copy of file: model.plan at path: "/opt/nim/
.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/model.plan"
INFO 2026-02-28 09:52:25.850 lib.rs:203] File: memory_footprint.json found in cache: "/opt/nim/.cache/ngc/hub/models--n
im--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/memory_footprint.json"
INFO 2026-02-28 09:52:25.850 public.rs:52] Skipping download, using cached copy of file: memory_footprint.json at path:
 "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/memory_
footprint.json"
INFO 2026-02-28 09:52:25.855 lib.rs:203] File: metadata.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvid
ia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/metadata.json"
INFO 2026-02-28 09:52:25.855 public.rs:52] Skipping download, using cached copy of file: metadata.json at path: "/opt/n
im/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/l40sx1-trt-fp16-k5no-6dx4q/metadata.json"
INFO 2026-02-28 09:52:25.861 lib.rs:203] File: tokenizer.json found in cache: "/opt/nim/.cache/ngc/hub/models--nim--nvi
dia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:52:25.872 nim_sdk.py:328] Using the workspace specified during init: /opt/nim/workspace
INFO 2026-02-28 09:52:25.861 public.rs:52] Skipping download, using cached copy of file: tokenizer.json at path: "/opt/
nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer.json"
INFO 2026-02-28 09:52:25.867 lib.rs:203] File: tokenizer_config.json found in cache: "/opt/nim/.cache/ngc/hub/models--n
im--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_config.json"
INFO 2026-02-28 09:52:25.872 nim_sdk.py:341] Materializing workspace to: /opt/nim/workspace
INFO 2026-02-28 09:52:26.913 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
INFO 2026-02-28 09:52:29.462 repository.py:154] Loaded tokenizer from /opt/nim/workspace/tokenizer
WARNING 2026-02-28 09:52:34.629 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:52:34.631 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:52:34.631 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:52:34.631 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:52:34.631 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:52:34.632 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
WARNING 2026-02-28 09:52:34.632 tensor_rt_model_builder.py:110] Failed to convert tensor logits profile shapes to tuples, using empty tuples instead
INFO 2026-02-28 09:52:34.723 tensor_rt_model_builder.py:151] Available TRT engine profiles: 
  - (max_batch_size=1, max_seq_length=8192)
  - (max_batch_size=8, max_seq_length=8192)
  - (max_batch_size=16, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=1024)
  - (max_batch_size=30, max_seq_length=2048)
  - (max_batch_size=30, max_seq_length=4096)
INFO 2026-02-28 09:52:34.724 repository.py:256] Selected TensorRT engine profile: profile_index=3 inputs=[TensorInfo(name='input_ids', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192), min_shapes=(1, 2)), TensorInfo(name='attention_mask', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192), min_shapes=(1, 2))] outputs=[TensorInfo(name='logits', dtype='float32', shape=(1, 1), opt_shapes=(), max_shapes=(), min_shapes=())]
INFO 2026-02-28 09:52:34.726 tensor_rt_model_builder.py:228] Setting number of models for '_nvidia_llama_3_2_nv_rerankqa_1b_v2_model' to: 1
INFO 2026-02-28 09:52:34.726 tensor_rt_model_builder.py:231] Using TensorRT max_batch_size: 30
INFO 2026-02-28 09:52:34.726 tensor_rt_model_builder.py:244] Python model successfully generated and written to: /opt/nim/tmp/run/triton-model-repository/_nvidia_llama_3_2_nv_rerankqa_1b_v2_model/1/model.py
INFO 2026-02-28 09:52:34.928 bls_model_builder.py:75] BLS model successfully generated and written to: /opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/model.py
INFO 2026-02-28 09:52:34.959 repository.py:529] Successfully saved model service config to /opt/nim/tmp/run/triton-model-repository/service_config.yaml
INFO 2026-02-28 09:52:34.959 repository.py:531] Sucessfully generated Triton Model Repository at /opt/nim/tmp/run/triton-model-repository
INFO 2026-02-28 09:52:36.190 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages']
I0228 09:52:36.194795 294 pinned_memory_manager.cc:277] "Pinned memory pool is created at '0x204c00000' with size 268435456"
I0228 09:52:36.195019 294 cuda_memory_manager.cc:107] "CUDA memory pool is created on device 0 with size 67108864"
I0228 09:52:36.204779 294 model_lifecycle.cc:473] "loading: _nvidia_llama_3_2_nv_rerankqa_1b_v2_model:1"
I0228 09:52:36.204874 294 model_lifecycle.cc:473] "loading: nvidia_llama_3_2_nv_rerankqa_1b_v2:1"
I0228 09:52:36.421995 294 tensorrt.cc:65] "TRITONBACKEND_Initialize: tensorrt"
I0228 09:52:36.422052 294 tensorrt.cc:75] "Triton TRITONBACKEND API version: 1.19"
I0228 09:52:36.422061 294 tensorrt.cc:81] "'tensorrt' TRITONBACKEND API version: 1.19"
I0228 09:52:36.422070 294 tensorrt.cc:105] "backend configuration:\n{\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"version-compatible\":\"true\",\"default-max-batch-size\":\"4\"}}"
I0228 09:52:36.424380 294 tensorrt.cc:231] "TRITONBACKEND_ModelInitialize: _nvidia_llama_3_2_nv_rerankqa_1b_v2_model (version 1)"
INFO 2026-02-28 09:52:36.555 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:36.856 http_api.py:52] Serving endpoints:
  0.0.0.0:8000/v1/triton-inference-statistics (GET)
  0.0.0.0:8000/v1/models (GET)
  0.0.0.0:8000/v1/ranking (POST)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/metrics (GET)
  0.0.0.0:8000/v1/license (GET)
  0.0.0.0:8000/v1/metadata (GET)
  0.0.0.0:8000/v1/manifest (GET)
INFO 2026-02-28 09:52:36.856 http_api.py:73] {'message': 'Starting HTTP Inference server', 'port': 8000, 'workers_count': 8, 'host': '0.0.0.0', 'log_level': 'info', 'SSL': 'disabled'}
INFO 2026-02-28 09:52:25.867 public.rs:52] Skipping download, using cached copy of file: tokenizer_config.json at path:
 "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/tokenizer_
config.json"
INFO 2026-02-28 09:52:25.872 lib.rs:203] File: special_tokens_map.json found in cache: "/opt/nim/.cache/ngc/hub/models-
-nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_tokens_map.json"
INFO 2026-02-28 09:52:25.872 public.rs:52] Skipping download, using cached copy of file: special_tokens_map.json at pat
h: "/opt/nim/.cache/ngc/hub/models--nim--nvidia--llama-3.2-nv-rerankqa-1b-v2/snapshots/tokenizer-8192-3fe66485/special_
tokens_map.json"
2026-02-28T09:52:25Z INFO: nimlib.nim_sdk - Using the workspace specified during init: /opt/nim/workspace
2026-02-28T09:52:25Z INFO: nimlib.nim_sdk - Materializing workspace to: /opt/nim/workspace
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:52:29Z INFO: nemo-retriever-reranking-triton-gen - Loaded tokenizer from /opt/nim/workspace/tokenizer
2026-02-28T09:52:34Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:52:34Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:52:34Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:52:34Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:52:34Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:52:34Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:52:34Z WARNING: nemo-retriever-reranking-triton-gen - Failed to convert tensor logits profile shapes to t
uples, using empty tuples instead
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - Available TRT engine profiles: 
  - (max_batch_size=1, max_seq_length=8192)
  - (max_batch_size=8, max_seq_length=8192)
  - (max_batch_size=16, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=8192)
  - (max_batch_size=30, max_seq_length=1024)
  - (max_batch_size=30, max_seq_length=2048)
  - (max_batch_size=30, max_seq_length=4096)
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - Selected TensorRT engine profile: profile_index=3 inpu
ts=[TensorInfo(name='input_ids', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192), min_shapes
=(1, 2)), TensorInfo(name='attention_mask', dtype='int64', shape=(-1, -1), opt_shapes=(16, 512), max_shapes=(30, 8192),
 min_shapes=(1, 2))] outputs=[TensorInfo(name='logits', dtype='float32', shape=(1, 1), opt_shapes=(), max_shapes=(), mi
n_shapes=())]
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - Setting number of models for '_nvidia_llama_3_2_nv_rer
ankqa_1b_v2_model' to: 1
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - Using TensorRT max_batch_size: 30
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - Python model successfully generated and written to: /o
pt/nim/tmp/run/triton-model-repository/_nvidia_llama_3_2_nv_rerankqa_1b_v2_model/1/model.py
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - BLS model successfully generated and written to: /opt/
nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/model.py
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - Successfully saved model service config to /opt/nim/tm
p/run/triton-model-repository/service_config.yaml
2026-02-28T09:52:34Z INFO: nemo-retriever-reranking-triton-gen - Sucessfully generated Triton Model Repository at /opt/
nim/tmp/run/triton-model-repository
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
INFO 2026-02-28 09:52:38.275 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.275 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:38.299 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.299 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:38.357 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.357 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:38.360 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.360 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:36Z INFO: nimlib - Appending: /opt/nim to PYTHONPATH: ['/usr/local/bin', '/usr/lib/python312.zip', '/u
sr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dis
t-packages']
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:52:36Z INFO: nimlib.nim_inference_api_builder.api - Serving endpoints:
  0.0.0.0:8000/v1/triton-inference-statistics (GET)
  0.0.0.0:8000/v1/models (GET)
  0.0.0.0:8000/v1/ranking (POST)
  0.0.0.0:8000/v1/health/live (GET)
  0.0.0.0:8000/v1/health/ready (GET)
  0.0.0.0:8000/v1/metrics (GET)
  0.0.0.0:8000/v1/license (GET)
  0.0.0.0:8000/v1/metadata (GET)
  0.0.0.0:8000/v1/manifest (GET)
2026-02-28T09:52:36Z INFO: nimlib.nim_inference_api_builder.api - {'message': 'Starting HTTP Inference server', 'port':
 8000, 'workers_count': 8, 'host': '0.0.0.0', 'log_level': 'info', 'SSL': 'disabled'}
2026-02-28T09:52:36Z INFO: uvicorn.error - Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
2026-02-28T09:52:36Z INFO: uvicorn.error - Started parent process [295]
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [384]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [385]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [380]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [379]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [378]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [381]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [383]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: uvicorn.error - Started server process [382]
2026-02-28T09:52:38Z INFO: uvicorn.error - Waiting for application startup.
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: nimlib.profiles - Registered custom profile selectors: []
2026-02-28T09:52:38Z INFO: nimlib.profiles - Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7
c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
2026-02-28T09:52:38Z INFO: root - Loading service config from /opt/nim/tmp/run/triton-model-repository/service_config.y
aml
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
2026-02-28T09:52:38Z INFO: uvicorn.error - Application startup complete.
INFO 2026-02-28 09:52:38.361 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.361 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:38.373 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.374 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:38.374 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.375 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
INFO 2026-02-28 09:52:38.377 profiles.py:98] Registered custom profile selectors: []
INFO 2026-02-28 09:52:38.378 profiles.py:208] Matched profile_id in manifest from env NIM_MODEL_PROFILE to: 4aa05527bed7c338689c9418e81d029fa03a2b58569aedda792e8d5af6abe91c
I0228 09:52:39.814670 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_6 (CPU device 0)"
I0228 09:52:39.814588 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_0 (CPU device 0)"
I0228 09:52:39.814769 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_1 (CPU device 0)"
I0228 09:52:39.815738 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_4 (CPU device 0)"
I0228 09:52:39.815782 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_3 (CPU device 0)"
I0228 09:52:39.815832 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_7 (CPU device 0)"
I0228 09:52:39.815854 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_2 (CPU device 0)"
I0228 09:52:39.815866 294 python_be.cc:2289] "TRITONBACKEND_ModelInstanceInitialize: nvidia_llama_3_2_nv_rerankqa_1b_v2_0_5 (CPU device 0)"
INFO 2026-02-28 09:52:40.359 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:40.359 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:40.360 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:40.362 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:40.393 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:40.434 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:40.560 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
INFO 2026-02-28 09:52:40.683 __init__.py:413] Appending: /opt/nim to PYTHONPATH: ['', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1', '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2', '/opt/tritonserver/backends/python']
2026-02-28T09:52:40Z INFO: uvicorn.access - 172.17.0.1:42148 - "GET /v1/health/ready HTTP/1.1" 503
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
OTEL Logging handler requested, but Python logging auto-instrumentation not set up. Set OTEL_PYTHON_LOGGING_AUTO_INSTRU
MENTATION_ENABLED=true to enable logging auto-instrumentation.
2026-02-28T09:52:45Z INFO: uvicorn.access - 172.17.0.1:42148 - "GET /v1/health/ready HTTP/1.1" 503
I0228 09:52:45.926731 701 pb_stub.cc:2075]  Non-graceful termination detected. 
I0228 09:52:46.052000 717 pb_stub.cc:2075] I0228 09:52:46.052013 677 pb_stub.cc:2075] I0228 09:52:46.052008 676 pb_stub
.cc:2075]    Non-graceful termination detected. Non-graceful termination detected. Non-graceful termination detected. 


I0228 09:52:46.058487 733 pb_stub.cc:2075] I0228 09:52:46.058486 749 pb_stub.cc:2075]   Non-graceful termination detect
ed. Non-graceful termination detected. 

2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"reranking_model_name":{"string_value":"_nvidia_llama_3_
2_nv_rerankqa_1b_v2_model"},"tokenizer_template":{"string_value":"question:{query} \\n \\n passage:{passage}"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_6', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"reranking_model_name":{"string_value":"_nvidia_llama_3_
2_nv_rerankqa_1b_v2_model"},"tokenizer_template":{"string_value":"question:{query} \\n \\n passage:{passage}"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_7', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"reranking_model_name":{"string_value":"_nvidia_llama_3_
2_nv_rerankqa_1b_v2_model"},"tokenizer_template":{"string_value":"question:{query} \\n \\n passage:{passage}"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_5', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"reranking_model_name":{"string_value":"_nvidia_llama_3_
2_nv_rerankqa_1b_v2_model"},"tokenizer_template":{"string_value":"question:{query} \\n \\n passage:{passage}"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_2', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"reranking_model_name":{"string_value":"_nvidia_llama_3_
2_nv_rerankqa_1b_v2_model"},"tokenizer_template":{"string_value":"question:{query} \\n \\n passage:{passage}"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_0', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"reranking_model_name":{"string_value":"_nvidia_llama_3_
2_nv_rerankqa_1b_v2_model"},"tokenizer_template":{"string_value":"question:{query} \\n \\n passage:{passage}"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_4', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Initializing RerankingBlsModel with args: {'
model_config': '{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2","platform":"","backend":"python","runtime":"","version_pol
icy":{"latest":{"num_versions":1}},"max_batch_size":0,"input":[{"name":"query","data_type":"TYPE_STRING","format":"FORM
AT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged_batch":false,"optional":false,"is_non_linear_format_io":fals
e},{"name":"passage","data_type":"TYPE_STRING","format":"FORMAT_NONE","dims":[-1],"is_shape_tensor":false,"allow_ragged
_batch":false,"optional":false,"is_non_linear_format_io":false}],"output":[{"name":"index","data_type":"TYPE_INT32","di
ms":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"logit","data_type":"TYPE
_FP32","dims":[-1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false},{"name":"token_count","
data_type":"TYPE_INT32","dims":[1],"label_filename":"","is_shape_tensor":false,"is_non_linear_format_io":false}],"batch
_input":[],"batch_output":[],"optimization":{"priority":"PRIORITY_DEFAULT","input_pinned_memory":{"enable":true},"outpu
t_pinned_memory":{"enable":true},"gather_kernel_buffer_threshold":0,"eager_batching":false},"dynamic_batching":{"prefer
red_batch_size":[],"max_queue_delay_microseconds":100,"preserve_ordering":false,"priority_levels":0,"default_priority_l
evel":0,"priority_queue_policy":{}},"instance_group":[{"name":"nvidia_llama_3_2_nv_rerankqa_1b_v2_0","kind":"KIND_CPU",
"count":8,"gpus":[],"secondary_devices":[],"profile":[],"passive":false,"host_policy":""}],"default_model_filename":"mo
del.py","cc_model_filenames":{},"metric_tags":{},"parameters":{"reranking_model_name":{"string_value":"_nvidia_llama_3_
2_nv_rerankqa_1b_v2_model"},"tokenizer_template":{"string_value":"question:{query} \\n \\n passage:{passage}"}},"model_
warmup":[]}', 'model_instance_kind': 'CPU', 'model_instance_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2_0_3', 'model_ins
tance_device_id': '0', 'model_repository': '/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2
', 'model_version': '1', 'model_name': 'nvidia_llama_3_2_nv_rerankqa_1b_v2'}
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Bls config: hf_model=None base_path=None tok
enizer_path=None tokenizer_template='question:{query} \n \n passage:{passage}' max_seq_length=None reranking_model_name
='_nvidia_llama_3_2_nv_rerankqa_1b_v2_model'
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.bls - Tokenizer config: hf_model=None base_path=Po
sixPath('/opt/nim/tmp/run/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1') tokenizer_path=None tokenizer_
template='question:{query} \n \n passage:{passage}' max_seq_length=None
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
2026-02-28T09:52:46Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Loading tokenizer from /opt/nim/tmp/ru
n/triton-model-repository/nvidia_llama_3_2_nv_rerankqa_1b_v2/1/tokenizer
I0228 09:52:46.139280 765 pb_stub.cc:2075]  Non-graceful termination detected. 
I0228 09:52:46.139656 686 pb_stub.cc:2075]  Non-graceful termination detected. 
terminate called without an active exception
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
terminate called without an active exception
terminate called without an active exception
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
terminate called without an active exception
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
terminate called without an active exception
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - Tokenizer is fast: True
2026-02-28T09:52:47Z INFO: nemo_retriever_reranking_triton_gen.tools.tokenizer - tokenizer.model_max_length: 8192
2026-02-28T09:52:49Z INFO: uvicorn.access - 172.17.0.1:42148 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:52:53Z INFO: uvicorn.access - 172.17.0.1:42148 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:52:57Z INFO: uvicorn.access - 172.17.0.1:42148 - "GET /v1/health/ready HTTP/1.1" 503
2026-02-28T09:53:01Z INFO: uvicorn.access - 172.17.0.1:42148 - "GET /v1/health/ready HTTP/1.1" 503
